{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Testing the approach described in [this](https://arxiv.org/abs/1912.02175/) paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import time as time\n",
    "import blackbox_backprop as bb\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bb_net(nn.Module):\n",
    "    '''\n",
    "    This net is equipped to run an m-by-m grid graphs. No A matrix is necessary.\n",
    "    '''\n",
    "    def __init__(self, m, context_size, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.device = device\n",
    "        self.hidden_dim = 2*context_size\n",
    "        # self.shortestPath = bb.ShortestPath()\n",
    "\n",
    "        ## Standard layers\n",
    "        self.fc_1 = nn.Linear(context_size, self.hidden_dim)\n",
    "        self.fc_2 = nn.Linear(self.hidden_dim, self.m**2)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "        \n",
    "    def forward(self, d):\n",
    "        w = self.leaky_relu(self.fc_1(d))\n",
    "        w = self.fc_2(w)\n",
    "        suggested_weights = w.view(w.shape[0], self.m, self.m)\n",
    "        suggested_shortest_paths = bb.ShortestPath.apply(suggested_weights, 5.0)\n",
    "        \n",
    "        return suggested_shortest_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BB_net = bb_net(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "grid_size = 5\n",
    "data_path = './shortest_path_data/Shortest_Path_training_data'+str(grid_size)+'.pth'\n",
    "state = torch.load(data_path)\n",
    "\n",
    "## Extract data from state\n",
    "train_dataset_e = state['train_dataset_e']\n",
    "test_dataset_e = state['test_dataset_e']\n",
    "train_dataset_v = state['train_dataset_v']\n",
    "test_dataset_v = state['test_dataset_v']\n",
    "m = state[\"m\"]\n",
    "A = state[\"A\"].float()\n",
    "b = state[\"b\"].float()\n",
    "WW = state[\"WW\"].float()\n",
    "num_edges = state[\"num_edges\"]\n",
    "Edge_list = state[\"Edge_list\"]\n",
    "Edge_list_torch = torch.tensor(Edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training setup\n",
    "train_dataset = train_dataset_v\n",
    "test_dataset = test_dataset_v\n",
    "net = BB_net\n",
    "learning_rate = 1e-3\n",
    "\n",
    "test_size = 200\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=100,\n",
    "                              shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=test_size,\n",
    "                             shuffle=False)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "## Initialize arrays that will be returned.\n",
    "test_loss_hist= []\n",
    "test_acc_hist = []\n",
    "train_time = [0]\n",
    "train_loss_ave = 0\n",
    "max_time = 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HammingLoss(torch.nn.Module):\n",
    "    def forward(self, suggested, target):\n",
    "        errors = suggested * (1.0 - target) + (1.0 - suggested) * target\n",
    "        return errors.mean(dim=0).sum()\n",
    "        # return (torch.mean(suggested*(1.0-target)) + torch.mean((1.0-suggested)*target)) * 25.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the lost function used in the original paper. We can try it, but it doesn't seem to \n",
    "# perform significantly better then nn.MSE\n",
    "# criterion = HammingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , test_loss =  0.27399998903274536\n",
      "epoch: 1 , test_loss =  0.27399998903274536\n",
      "epoch: 2 , test_loss =  0.27399998903274536\n",
      "epoch: 3 , test_loss =  0.27399998903274536\n",
      "epoch: 4 , test_loss =  0.27399998903274536\n",
      "epoch: 5 , test_loss =  0.27399998903274536\n",
      "epoch: 6 , test_loss =  0.27399998903274536\n",
      "epoch: 7 , test_loss =  0.27399998903274536\n",
      "epoch: 8 , test_loss =  0.27399998903274536\n",
      "epoch: 9 , test_loss =  0.27399998903274536\n",
      "epoch: 10 , test_loss =  0.27399998903274536\n",
      "epoch: 11 , test_loss =  0.27399998903274536\n",
      "epoch: 12 , test_loss =  0.27399998903274536\n",
      "epoch: 13 , test_loss =  0.27399998903274536\n",
      "epoch: 14 , test_loss =  0.27399998903274536\n",
      "epoch: 15 , test_loss =  0.27399998903274536\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-50a8df77cd72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuggested_shortest_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtrain_loss_ave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Train!\n",
    "max_epochs = 100\n",
    "device = 'cuda:0'\n",
    "net.to(device)\n",
    "hammingLoss = HammingLoss()\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    net.train()\n",
    "    for d_batch, path_batch in train_loader:\n",
    "            d_batch = d_batch.to(device)\n",
    "            path_batch =path_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            suggested_shortest_paths = net(d_batch)\n",
    "            # suggested_shortest_paths = shortestPath.apply(suggested_weights, 5.0) # Set the lambda hyperparameter\n",
    "            # print(suggested_shortest_paths.shape)\n",
    "            # loss = hammingLoss(suggested_shortest_paths, path_batch)\n",
    "            loss = criterion(suggested_shortest_paths, path_batch)\n",
    "            train_loss_ave = loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    net.eval()\n",
    "    for d_batch, path_batch in test_loader:\n",
    "        d_batch = d_batch.to(device)\n",
    "        path_batch =path_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        suggested_shortest_paths = net(d_batch)\n",
    "        loss = criterion(suggested_shortest_paths, path_batch)\n",
    "        test_loss = loss.item()\n",
    "        scheduler.step(test_loss)\n",
    "        print('epoch:', epoch, ', test_loss = ', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_batch, path_batch = next(iter(test_loader))\n",
    "d_batch = d_batch.to(device)\n",
    "path_batch = path_batch.to(device)\n",
    "pred_batch = BB_net(d_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "WW = WW.to(device)\n",
    "sp = bb.ShortestPath()\n",
    "\n",
    "for i in range(10):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.matshow(pred_batch[i,:,:].cpu().detach().numpy())\n",
    "    ax2.matshow(path_batch[i,:,:].cpu().detach().numpy())\n",
    "    #plt.matshow(pred_batch[i,:,:].cpu().detach().numpy())\n",
    "    #plt.show()\n",
    "    #plt.matshow(path_batch[i,:,:].cpu().detach().numpy())\n",
    "    #plt.show()\n",
    "    #weights = torch.matmul(WW, d_batch[i]).view((1, 5, 5))\n",
    "    # print(weights)\n",
    "    #pred_sp = sp.apply(weights,20)\n",
    "    #plt.matshow(pred_sp[0,:,:].cpu().detach().numpy())\n",
    "    #plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WW.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
